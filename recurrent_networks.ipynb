{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new Tensorflow thingy",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz5XCstyR-D1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "4234d13c-a34e-452c-f786-c87158551796"
      },
      "source": [
        "!pip install msgpack_numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting msgpack_numpy\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/32/323eda6da56cdbf768e41858d491c163a6989f27b1733eb3e9fca21291aa/msgpack_numpy-0.4.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from msgpack_numpy) (1.18.4)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from msgpack_numpy) (1.0.0)\n",
            "Installing collected packages: msgpack-numpy\n",
            "Successfully installed msgpack-numpy-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW3ZZpMVJ-lZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a52666b-11f0-490d-8e9b-a3e50d3b885d"
      },
      "source": [
        "import msgpack\n",
        "import msgpack_numpy\n",
        "import numpy as np\n",
        "from sys import argv\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "%load_ext tensorboard\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "print(\"Num GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
        "\n",
        "#enable numpy in msgpack files\n",
        "msgpack_numpy.patch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqBFwKOkKGMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "ac2609d8-cf7a-40f2-f4a2-e13a292ccd63"
      },
      "source": [
        "featurevector_length= 10\n",
        "\n",
        "group_pointclouds = 2\n",
        "sequence_length = 50\n",
        "\n",
        "files = [#\"adult_EWI-25.msgpack\",\n",
        "        #\"achtertuinheuvel-1.msgpack\" ,\n",
        "         #\"EWI_2_avond-25.msgpack\",\n",
        "       #  \"EWI_3-26.msgpack\" ,\n",
        "         #\"EWI_solarpanel-29.msgpack\",\n",
        "        #\"schoolpleinheuvel-1.msgpack\",\n",
        "     #(\"ewitest-18.msgpack\",2.0,0),\n",
        "    (\"football_children.msgpack\",1.80,-3.3),\n",
        "    (\"football_2-31.msgpack\",1.80,-3.3),\n",
        "    (\"football_3-31.msgpack\",1.80,-3.3),\n",
        "    (\"adults-31.msgpack\",1.80,-3.3),\n",
        "    (\"fietsen-20.msgpack\",2.0,-2.6),\n",
        "    (\"fietsen2-20.msgpack\",2.0,-2.6),\n",
        "    #(\"one_at_a_time-31.msgpack\",1.80,-3.3),\n",
        "    (\"mixed-31.msgpack\",1.80,-3.3),\n",
        "]\n",
        "\n",
        "def pol2cart(rho, phi):\n",
        "    x = rho * np.cos(phi)\n",
        "    y = rho * np.sin(phi)\n",
        "    return(x, y)\n",
        "\n",
        "def get_featurevector(data, height, angle):\n",
        "    \"\"\"\n",
        "     Data = [range, angle, doppler, elevation, snr]\n",
        "    \"\"\"\n",
        "    #print(data)\n",
        "    #points = np.sum((np.sum(data, axis=2) != 0), axis=1)\n",
        "    points = data.shape[0]\n",
        "\n",
        "    summed = np.sum(data, axis=0)\n",
        "    averaged = np.mean(data,axis=0)\n",
        "    #deviation = np.std(data, axis=1)\n",
        "\n",
        "    x, y = pol2cart(data[:,0], data[:,1])\n",
        "    # featurevecs = np.zeros((featurevector_length))\n",
        "\n",
        "    # _, elevation = pol2cart(data[:,0], data[:,4]+(3.1415/180*angle))\n",
        "    # elevation += height\n",
        "    # featurevecs[0] = averaged[0]\n",
        "    # featurevecs[1] = averaged[1]\n",
        "    # featurevecs[2] = averaged[2]\n",
        "    # featurevecs[3] = np.percentile(elevation, 95)\n",
        "    # featurevecs[4] = averaged[3]\n",
        "    # featurevecs[5] = np.percentile(elevation, 5)\n",
        "    # featurevecs[6] = np.std(x)#deviation[0]\n",
        "    # featurevecs[7] = np.std(y)#deviation[1]\n",
        "    # #featurevecs[6] = 0#np.mean(elevation)#np.mean(data[:,3]) / ((1/(averaged[0]/1400) +130)) if averaged[0] > 6 else (np.mean(data[:,3])/360)\n",
        "    # #featurevecs[7 ] = points\n",
        "    # #Out: [num points, range, angle, doppler, snr tot, snr avg, angle stdev, doppler stdev, rangedev, snr stdev ]\n",
        "\n",
        "    #COPIED FROM RFCLASSIFIER\n",
        "    variance = np.var(data, axis=0)\n",
        "\n",
        "    featurevecs = np.zeros((featurevector_length))\n",
        "\n",
        "    x, y = pol2cart(data[:,0], data[:,1])\n",
        "\n",
        "    _, elevation = pol2cart(data[:,0], data[:,4]+(3.1415/180*angle))\n",
        "    elevation += height\n",
        "    featurevecs[0] = averaged[0] #range\n",
        "    featurevecs[1] = averaged[1] #angle\n",
        "    featurevecs[2] = averaged[2] #doppler\n",
        "    featurevecs[3] = np.percentile(elevation, 95)\n",
        "    #featurevecs[10] = np.sum(data[:,3], ) #snr\n",
        "\n",
        "    featurevecs[4] = np.std(x)#deviation[0]\n",
        "    featurevecs[5] = np.std(y)#deviation[1]\n",
        "    #featurevecs[7] = deviation[2]\n",
        "    #featurevecs[8] = deviation[3]\n",
        "\n",
        "    featurevecs[6:9] = variance[0:3]\n",
        "\n",
        "    # featurevecs[10] = np.percentile(elevation, 95)\n",
        "    # featurevecs[11] = np.percentile(elevation, 5)\n",
        "\n",
        "    return featurevecs\n",
        "\n",
        "\n",
        "def read_file(filename):\n",
        "    \"\"\"\n",
        "    read a messagepack file and return individual messages\n",
        "    :return: \n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as file:\n",
        "        unpacker = msgpack.Unpacker(file, raw=False)\n",
        "        for msg in unpacker:\n",
        "            yield msg\n",
        "\n",
        "\n",
        "def get_pointclouds(msg):\n",
        "    \"\"\"\n",
        "    get pointcloud data from msg\n",
        "    :param msg:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return msg['pointclouds']\n",
        "\n",
        "\n",
        "def get_dataset(fileinfo, id):\n",
        "    feature_vectors = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    filename = fileinfo[0]\n",
        "    sensorheight = fileinfo[1]\n",
        "    sensorangle = fileinfo[2]\n",
        "    for msg in read_file(filename):\n",
        "        msg_feature_vectors = []\n",
        "        msg_labels = 0\n",
        "        pointclouds = get_pointclouds(msg)\n",
        "        if(len(pointclouds)  > 100):\n",
        "            class_id = msg['class_id']\n",
        "            if(class_id >=0):\n",
        "                i = 0\n",
        "                sequence =[]\n",
        "                while i < len(pointclouds):\n",
        "                    pointcloud = pointclouds[i]\n",
        "                    i += 1\n",
        "                    if (pointcloud.shape[0] > 1):\n",
        "                        for j in range(min(group_pointclouds - 1, len(pointclouds)-i)):\n",
        "                            pointcloud2 = pointclouds[i]\n",
        "                            if (pointcloud2.shape[0] > 1):\n",
        "                                pointcloud = np.append(pointcloud, pointcloud2, axis=0)\n",
        "                            i += 1\n",
        "                        fv = get_featurevector(pointcloud, sensorheight, sensorangle)\n",
        "                        sequence.append(fv)\n",
        "\n",
        "                if(len(sequence) >= sequence_length):\n",
        "                    for j in range(0,len(sequence)-sequence_length-1):\n",
        "                        feature_vectors.append(np.array(sequence[j : j+sequence_length]))\n",
        "                        labels.append(msg['class_id'] if msg['class_id'] >= 0 else 3)\n",
        "                        ids.append(id*100000+msg['uid'])\n",
        "\n",
        "    # labels: [adult, bike, child, clutter]\n",
        "\n",
        "    labels = np.array(labels)\n",
        "    features = np.array(feature_vectors)\n",
        "    ids = np.array(ids)\n",
        "    return labels, features, ids\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "ids = []\n",
        "for j in range(0, len(files), 1):\n",
        "    a, b, c = get_dataset(files[j],j)\n",
        "    features.append(b)\n",
        "    labels.append(a)\n",
        "    ids.append(c)\n",
        "    print(b.shape)\n",
        "\n",
        "#print(features)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "features = np.concatenate(features, axis=0)\n",
        "ids = np.concatenate(ids, axis=0)\n",
        "\n",
        "#b = tf.keras.utils.normalize(b, axis=0, order=2)\n",
        "print(features.shape, labels.shape)\n",
        "print(np.unique(labels,return_counts=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1735, 50, 10)\n",
            "(47, 50, 10)\n",
            "(1532, 50, 10)\n",
            "(2105, 50, 10)\n",
            "(1583, 50, 10)\n",
            "(670, 50, 10)\n",
            "(2811, 50, 10)\n",
            "(10483, 50, 10) (10483,)\n",
            "(array([0, 1, 2]), array([4099, 2253, 4131]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBvd_eKqREIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "34f9f3f1-5983-4e82-e2f0-16b2a6c44586"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(16),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(16, input_shape=(sequence_length,featurevector_length)),\n",
        "    #tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(3, activation='softmax'),    \n",
        "])\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 50000\n",
        "\n",
        "dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "test_dataset = dataset.take(1024).batch(BATCH_SIZE)\n",
        "train_dataset = dataset.skip(1024).batch(BATCH_SIZE)\n",
        "\n",
        "    \n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)    \n",
        "model.compile(  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002),\n",
        "                loss=loss,\n",
        "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "\n",
        "val_files = [\n",
        "    (\"one_at_a_time-31.msgpack\",1.80,-3.3),\n",
        "    (\"fietsen4-20.msgpack\",2.0,-2.6),\n",
        "    #(\"mixed-31.msgpack\",1.80,-3.3),\n",
        " ]\n",
        "val_features = []\n",
        "val_labels = []\n",
        "for j in range(0, len(val_files), 1):\n",
        "    a, b, c = get_dataset(val_files[j],j)\n",
        "    val_features.append(b)\n",
        "    val_labels.append(a)\n",
        "    print(b.shape)\n",
        "\n",
        "val_labels = np.concatenate(val_labels, axis=0)\n",
        "val_features = np.concatenate(val_features, axis=0)\n",
        "\n",
        "validation = tf.data.Dataset.from_tensor_slices((val_features,val_labels)).batch(BATCH_SIZE)\n",
        "\n",
        "# #model.load_weights('easy_checkpoint')\n",
        "model.fit(train_dataset, epochs=15, validation_data=validation, callbacks=[tensorboard_callback])\n",
        "#model.save_weights('easy_checkpoint')\n",
        "# res = model(b)\n",
        "# #print(a.shape, np.argmax(res.numpy(),axis=1).shape)\n",
        "# print(tf.math.confusion_matrix(np.argmax(a,axis=1), np.argmax(res.numpy(),axis=1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3360, 50, 10)\n",
            "(431, 50, 10)\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Layer batch_normalization_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "148/148 [==============================] - 3s 19ms/step - loss: 1.0307 - sparse_categorical_accuracy: 0.5796 - val_loss: 1.0047 - val_sparse_categorical_accuracy: 0.6737\n",
            "Epoch 2/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.9140 - sparse_categorical_accuracy: 0.7191 - val_loss: 0.8492 - val_sparse_categorical_accuracy: 0.8430\n",
            "Epoch 3/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.8017 - sparse_categorical_accuracy: 0.8217 - val_loss: 0.7032 - val_sparse_categorical_accuracy: 0.9114\n",
            "Epoch 4/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.7205 - sparse_categorical_accuracy: 0.8764 - val_loss: 0.6688 - val_sparse_categorical_accuracy: 0.9090\n",
            "Epoch 5/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6889 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.6873 - val_sparse_categorical_accuracy: 0.8784\n",
            "Epoch 6/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6633 - sparse_categorical_accuracy: 0.9102 - val_loss: 0.6523 - val_sparse_categorical_accuracy: 0.9148\n",
            "Epoch 7/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6528 - sparse_categorical_accuracy: 0.9187 - val_loss: 0.6477 - val_sparse_categorical_accuracy: 0.9164\n",
            "Epoch 8/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6439 - sparse_categorical_accuracy: 0.9232 - val_loss: 0.6495 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 9/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6336 - sparse_categorical_accuracy: 0.9317 - val_loss: 0.6486 - val_sparse_categorical_accuracy: 0.9145\n",
            "Epoch 10/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6274 - sparse_categorical_accuracy: 0.9360 - val_loss: 0.6486 - val_sparse_categorical_accuracy: 0.9116\n",
            "Epoch 11/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6251 - sparse_categorical_accuracy: 0.9379 - val_loss: 0.6466 - val_sparse_categorical_accuracy: 0.9119\n",
            "Epoch 12/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6246 - sparse_categorical_accuracy: 0.9350 - val_loss: 0.6450 - val_sparse_categorical_accuracy: 0.9127\n",
            "Epoch 13/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6216 - sparse_categorical_accuracy: 0.9391 - val_loss: 0.6447 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 14/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6201 - sparse_categorical_accuracy: 0.9380 - val_loss: 0.6445 - val_sparse_categorical_accuracy: 0.9090\n",
            "Epoch 15/15\n",
            "148/148 [==============================] - 2s 15ms/step - loss: 0.6185 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.6436 - val_sparse_categorical_accuracy: 0.9116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9ef0623358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSTVFC3asr92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6a632443-3dd3-4ce4-8365-b8d3b3ef89be"
      },
      "source": [
        "\n",
        "predicted = np.argmax(model(val_features), axis=1)\n",
        "print(np.sum(predicted == val_labels) / len(val_labels))\n",
        "tf.math.confusion_matrix(val_labels, predicted,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9105776839883936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
              "array([[1763,    0,   50],\n",
              "       [  32,  398,    1],\n",
              "       [ 256,    0, 1291]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPwTg8j4Lnrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "4da4097a-0b7f-4bb6-d79d-576e5f84707b"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzqvQ0cugSa_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "e048bd06-6642-4eaf-8c20-1a0bd0ef87e6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_4 (Batch multiple                  40        \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              multiple                  176       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  2112      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              multiple                  51        \n",
            "=================================================================\n",
            "Total params: 2,379\n",
            "Trainable params: 2,359\n",
            "Non-trainable params: 20\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifGkSV_ygQ3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDgbsIXOmJok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cdf1c2e7-2123-4af9-91c6-880696300e2e"
      },
      "source": [
        "\n",
        "val_files = [\n",
        "    (\"one_at_a_time-31.msgpack\",1.80,-3.3),\n",
        "    (\"fietsen4-20.msgpack\",2.0,-2.6),\n",
        " #   (\"mixed-31.msgpack\",1.80,-3.3),\n",
        " ]\n",
        "\n",
        "sequence_length=50\n",
        "\n",
        "def get_dataset(fileinfo, id):\n",
        "    feature_vectors = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    filename = fileinfo[0]\n",
        "    sensorheight = fileinfo[1]\n",
        "    sensorangle = fileinfo[2]\n",
        "    for msg in read_file(filename):\n",
        "        msg_feature_vectors = []\n",
        "        msg_labels = 0\n",
        "        pointclouds = get_pointclouds(msg)\n",
        "        if(len(pointclouds)  > 100):\n",
        "            class_id = msg['class_id']\n",
        "            if(class_id >=0):\n",
        "                i = 0\n",
        "                sequence =[]\n",
        "                while i < len(pointclouds):\n",
        "                    pointcloud = pointclouds[i]\n",
        "                    i += 1\n",
        "                    if (pointcloud.shape[0] > 1):\n",
        "                        for j in range(min(group_pointclouds - 1, len(pointclouds)-i)):\n",
        "                            pointcloud2 = pointclouds[i]\n",
        "                            if (pointcloud2.shape[0] > 1):\n",
        "                                pointcloud = np.append(pointcloud, pointcloud2, axis=0)\n",
        "                            i += 1\n",
        "                        fv = get_featurevector(pointcloud, sensorheight, sensorangle)\n",
        "                        sequence.append(fv)\n",
        "\n",
        "                if(len(sequence) >= sequence_length):\n",
        "                    for j in range(0,len(sequence)-sequence_length-1):\n",
        "                        feature_vectors.append(np.array(sequence[j : j+sequence_length]))\n",
        "                        labels.append(msg['class_id'] if msg['class_id'] >= 0 else 3)\n",
        "                        ids.append(id*100000+msg['uid'])\n",
        "\n",
        "    # labels: [adult, bike, child, clutter]\n",
        "\n",
        "    labels = np.array(labels)\n",
        "    features = np.array(feature_vectors)\n",
        "    ids = np.array(ids)\n",
        "    return labels, features, ids\n",
        "\n",
        "\n",
        "val_features = []\n",
        "val_labels = []\n",
        "for j in range(0, len(val_files), 1):\n",
        "    a, b, c = get_dataset(val_files[j],j)\n",
        "    val_features.append(b)\n",
        "    val_labels.append(a)\n",
        "    print(b.shape)\n",
        "\n",
        "val_labels = np.concatenate(val_labels, axis=0)\n",
        "val_features = np.concatenate(val_features, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3360, 50, 10)\n",
            "(431, 50, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ9laAbXCu-d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3765b8bb-4c3d-4100-8f83-22d9115f3a97"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo multiple                  40        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  704       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  24960     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  195       \n",
            "=================================================================\n",
            "Total params: 25,899\n",
            "Trainable params: 25,879\n",
            "Non-trainable params: 20\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmOZe4EDD_pW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ec2ea5a-ef01-49d3-9aed-88361f18fb1a"
      },
      "source": [
        "32+288+8320+99"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8739"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}